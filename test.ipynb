{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T01:00:51.277189Z",
     "start_time": "2024-12-18T00:56:07.182312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "############################################################\n",
    "# Character set and indexing\n",
    "############################################################\n",
    "character_set = list(\" aăâbcdđeêfghijklmnoôơpqrstuưvwxyzAĂÂBCDĐEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?\")\n",
    "if '<unk>' not in character_set:\n",
    "    character_set.insert(0, '<unk>')\n",
    "char2idx = {c: i for i, c in enumerate(character_set)}\n",
    "\n",
    "############################################################\n",
    "# Helper to compute mean and std of mel-spectrograms\n",
    "############################################################\n",
    "def compute_mel_stats(dataset_name, split, cache_dir=\"./dataset_cache\", n_mels=80, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Compute mean and std of mel spectrograms across the dataset.\n",
    "    For large datasets, this might be slow. Consider caching the results.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(dataset_name, split=split, cache_dir=cache_dir)\n",
    "    mel_transform = T.MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels)\n",
    "    sums = 0.0\n",
    "    squared_sums = 0.0\n",
    "    count = 0\n",
    "\n",
    "    print(\"Computing mel stats (mean/std) over the dataset...\")\n",
    "    for item in dataset:\n",
    "        audio_array = item[\"audio\"][\"array\"]\n",
    "        audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
    "        mel_spec = mel_transform(audio_tensor)  # [n_mels, time]\n",
    "        # Flatten mel_spec for mean/std computation\n",
    "        sums += mel_spec.sum().item()\n",
    "        squared_sums += (mel_spec ** 2).sum().item()\n",
    "        count += mel_spec.numel()\n",
    "\n",
    "    mean = sums / count\n",
    "    var = (squared_sums / count) - (mean ** 2)\n",
    "    std = math.sqrt(var)\n",
    "    print(f\"Computed mel stats: mean={mean:.4f}, std={std:.4f}\")\n",
    "    return mean, std\n",
    "\n",
    "############################################################\n",
    "# Dataset & DataLoader\n",
    "############################################################\n",
    "class TTSDataLoader:\n",
    "    def __init__(self, dataset_name=\"doof-ferb/vlsp2020_vinai_100h\", split=\"train\",\n",
    "                 cache_dir=\"./dataset_cache\", max_retries=5, retry_delay=5,\n",
    "                 mel_mean=None, mel_std=None, n_mels=80, sample_rate=16000):\n",
    "        \"\"\"\n",
    "        Class to load and preprocess the TTS dataset.\n",
    "        Downloads from Hugging Face if not cached.\n",
    "        Normalizes mel-spectrograms if mean and std are provided.\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "        self.cache_dir = cache_dir\n",
    "        self.max_retries = max_retries\n",
    "        self.retry_delay = retry_delay\n",
    "        self.n_mels = n_mels\n",
    "        self.sample_rate = sample_rate\n",
    "        self.dataset = self.load_dataset_with_retry()\n",
    "\n",
    "        # Mel transform\n",
    "        self.mel_transform = T.MelSpectrogram(sample_rate=self.sample_rate, n_mels=self.n_mels)\n",
    "\n",
    "        # Normalization params\n",
    "        self.mel_mean = mel_mean\n",
    "        self.mel_std = mel_std\n",
    "\n",
    "    def load_dataset_with_retry(self):\n",
    "        retries = 0\n",
    "        while retries < self.max_retries:\n",
    "            try:\n",
    "                dataset = load_dataset(self.dataset_name, split=self.split, cache_dir=self.cache_dir)\n",
    "                return dataset\n",
    "            except Exception as e:\n",
    "                if \"503\" in str(e) or \"ConnectionError\" in str(e):\n",
    "                    print(f\"Server error ({type(e).__name__}), retrying {retries + 1}/{self.max_retries} in {self.retry_delay}s...\")\n",
    "                    retries += 1\n",
    "                    time.sleep(self.retry_delay)\n",
    "                else:\n",
    "                    raise e\n",
    "        raise Exception(f\"Failed to load dataset after {self.max_retries} retries.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Return text_tensor and mel_spec for the given index.\n",
    "        text_tensor: LongTensor [text_length]\n",
    "        mel_spec: FloatTensor [n_mels, time_steps]\n",
    "        \"\"\"\n",
    "        item = self.dataset[index]\n",
    "        text = item[\"transcription\"]\n",
    "        audio_array = item[\"audio\"][\"array\"]\n",
    "\n",
    "        # Map characters to indices\n",
    "        text_indices = [char2idx.get(c, char2idx['<unk>']) for c in text]\n",
    "        text_tensor = torch.tensor(text_indices, dtype=torch.long)\n",
    "\n",
    "        # Convert audio to mel spectrogram\n",
    "        audio_tensor = torch.tensor(audio_array, dtype=torch.float32)\n",
    "        mel_spec = self.mel_transform(audio_tensor)\n",
    "        if self.mel_mean is not None and self.mel_std is not None:\n",
    "            mel_spec = (mel_spec - self.mel_mean) / (self.mel_std + 1e-5)\n",
    "\n",
    "        return text_tensor, mel_spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pad sequences in the batch for both text and mel specs.\n",
    "    Return padded_texts, padded_mels, text_lengths, mel_lengths.\n",
    "    \"\"\"\n",
    "    texts, mels = zip(*batch)\n",
    "    text_lengths = torch.tensor([len(text) for text in texts], dtype=torch.long)\n",
    "    mel_lengths = torch.tensor([mel.size(1) for mel in mels], dtype=torch.long)\n",
    "\n",
    "    max_text_length = text_lengths.max().item()\n",
    "    max_mel_length = mel_lengths.max().item()\n",
    "\n",
    "    # Pad text\n",
    "    padded_texts = torch.zeros(len(texts), max_text_length, dtype=torch.long)\n",
    "    for i, text in enumerate(texts):\n",
    "        padded_texts[i, :len(text)] = text\n",
    "\n",
    "    # Pad mel\n",
    "    padded_mels = torch.zeros(len(mels), mels[0].size(0), max_mel_length, dtype=torch.float32)\n",
    "    for i, mel in enumerate(mels):\n",
    "        padded_mels[i, :, :mel.size(1)] = mel\n",
    "\n",
    "    return padded_texts, padded_mels, text_lengths, mel_lengths\n",
    "\n",
    "############################################################\n",
    "# Model Components\n",
    "############################################################\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # [max_len, 1, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [seq_len, batch, d_model]\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=False\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        return self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=False\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        return self.transformer_decoder(\n",
    "            tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerTTS(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model=256, nhead=4, num_layers=4,\n",
    "                 dim_feedforward=1024, dropout=0.1):\n",
    "        super(TransformerTTS, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.text_embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        self.mel_embed = nn.Linear(output_dim, d_model)\n",
    "        self.pos_decoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        self.encoder = TransformerEncoder(d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "        self.decoder = TransformerDecoder(d_model, nhead, num_layers, dim_feedforward, dropout)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None):\n",
    "        # Embed text\n",
    "        src_embedded = self.text_embedding(src) * math.sqrt(self.d_model)\n",
    "        src_embedded = src_embedded.permute(1, 0, 2)  # [seq_len, batch, d_model]\n",
    "        src_embedded = self.pos_encoder(src_embedded)\n",
    "\n",
    "        # Embed mel\n",
    "        tgt_embedded = self.mel_embed(tgt.permute(0, 2, 1)) * math.sqrt(self.d_model)\n",
    "        tgt_embedded = tgt_embedded.permute(1, 0, 2)\n",
    "        tgt_embedded = self.pos_decoder(tgt_embedded)\n",
    "\n",
    "        memory = self.encoder(src_embedded, src_mask, src_key_padding_mask)\n",
    "        output = self.decoder(tgt_embedded, memory, tgt_mask, None, tgt_key_padding_mask, src_key_padding_mask)\n",
    "\n",
    "        output = self.fc_out(output)  # [tgt_seq_len, batch, output_dim]\n",
    "        output = output.permute(1, 2, 0)  # [batch, output_dim, seq_len]\n",
    "        return output\n",
    "\n",
    "############################################################\n",
    "# Training Function\n",
    "############################################################\n",
    "def train():\n",
    "    # Hyperparameters\n",
    "    input_dim = len(character_set)\n",
    "    output_dim = 80  # Mel bands\n",
    "    d_model = 256\n",
    "    nhead = 4\n",
    "    num_layers = 4           # Reduced from 6 to 4 for stability\n",
    "    dim_feedforward = 1024\n",
    "    dropout = 0.05\n",
    "    batch_size = 8\n",
    "    learning_rate = 5e-5     # Reduced LR from 1e-4 to 5e-5\n",
    "    num_epochs = 10\n",
    "    grad_clip = 0.5\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Compute mel stats (only do once and save if needed)\n",
    "    mel_mean, mel_std = compute_mel_stats(\"doof-ferb/vlsp2020_vinai_100h\", \"train\", n_mels=output_dim, sample_rate=16000)\n",
    "\n",
    "    # Initialize model, criterion, optimizer\n",
    "    model = TransformerTTS(\n",
    "        input_dim=input_dim,\n",
    "        output_dim=output_dim,\n",
    "        d_model=d_model,\n",
    "        nhead=nhead,\n",
    "        num_layers=num_layers,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "\n",
    "    # Use L1Loss for more stable training\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Initialize dataset & dataloader\n",
    "    dataset = TTSDataLoader(\n",
    "        split=\"train\",\n",
    "        mel_mean=mel_mean,\n",
    "        mel_std=mel_std,\n",
    "        n_mels=output_dim,\n",
    "        sample_rate=16000\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True, num_workers=4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for i, (src, tgt, src_lengths, tgt_lengths) in enumerate(dataloader):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            loss = criterion(output, tgt)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Step {i}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch} completed. Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"checkpoints/transformer_tts_epoch{epoch}.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "id": "2d5684a98c437e34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "200e8f1e58c144d4bcd78a59ed46c7bc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Resolving data files:   0%|          | 0/35 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcfb67a14a4e43af82097d08f02afeaf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0/35 [00:00<?, ?files/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "952d470f39c745b1973a80bf3a23fde5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/56427 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15608349025647bbad341a14e269383a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 305\u001B[0m\n\u001B[1;32m    301\u001B[0m         torch\u001B[38;5;241m.\u001B[39msave(model\u001B[38;5;241m.\u001B[39mstate_dict(), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpoints/transformer_tts_epoch\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 305\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[1], line 247\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m()\u001B[0m\n\u001B[1;32m    244\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    246\u001B[0m \u001B[38;5;66;03m# Compute mel stats (only do once and save if needed)\u001B[39;00m\n\u001B[0;32m--> 247\u001B[0m mel_mean, mel_std \u001B[38;5;241m=\u001B[39m \u001B[43mcompute_mel_stats\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdoof-ferb/vlsp2020_vinai_100h\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_mels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# Initialize model, criterion, optimizer\u001B[39;00m\n\u001B[1;32m    250\u001B[0m model \u001B[38;5;241m=\u001B[39m TransformerTTS(\n\u001B[1;32m    251\u001B[0m     input_dim\u001B[38;5;241m=\u001B[39minput_dim,\n\u001B[1;32m    252\u001B[0m     output_dim\u001B[38;5;241m=\u001B[39moutput_dim,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    257\u001B[0m     dropout\u001B[38;5;241m=\u001B[39mdropout,\n\u001B[1;32m    258\u001B[0m )\u001B[38;5;241m.\u001B[39mto(device)\n",
      "Cell \u001B[0;32mIn[1], line 27\u001B[0m, in \u001B[0;36mcompute_mel_stats\u001B[0;34m(dataset_name, split, cache_dir, n_mels, sample_rate)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_mel_stats\u001B[39m(dataset_name, split, cache_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./dataset_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m, n_mels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m80\u001B[39m, sample_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16000\u001B[39m):\n\u001B[1;32m     23\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m    Compute mean and std of mel spectrograms across the dataset.\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124;03m    For large datasets, this might be slow. Consider caching the results.\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m     mel_transform \u001B[38;5;241m=\u001B[39m T\u001B[38;5;241m.\u001B[39mMelSpectrogram(sample_rate\u001B[38;5;241m=\u001B[39msample_rate, n_mels\u001B[38;5;241m=\u001B[39mn_mels)\n\u001B[1;32m     29\u001B[0m     sums \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/datasets/load.py:2154\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[1;32m   2151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\u001B[38;5;241m.\u001B[39mas_streaming_dataset(split\u001B[38;5;241m=\u001B[39msplit)\n\u001B[1;32m   2153\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[0;32m-> 2154\u001B[0m \u001B[43mbuilder_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2160\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2162\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[1;32m   2163\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2164\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[1;32m   2165\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/datasets/builder.py:924\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    923\u001B[0m     prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[0;32m--> 924\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[1;32m    931\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/datasets/builder.py:1000\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[1;32m    996\u001B[0m split_dict\u001B[38;5;241m.\u001B[39madd(split_generator\u001B[38;5;241m.\u001B[39msplit_info)\n\u001B[1;32m    998\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    999\u001B[0m     \u001B[38;5;66;03m# Prepare split will record examples associated to the split\u001B[39;00m\n\u001B[0;32m-> 1000\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1001\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1002\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[1;32m   1003\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find data file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1004\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1005\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mOriginal error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1006\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[1;32m   1007\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/datasets/builder.py:1741\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split\u001B[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001B[0m\n\u001B[1;32m   1739\u001B[0m job_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1740\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pbar:\n\u001B[0;32m-> 1741\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mjob_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_split_single\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1742\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgen_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgen_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjob_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjob_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_prepare_split_args\u001B[49m\n\u001B[1;32m   1743\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1745\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/datasets/builder.py:1870\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split_single\u001B[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001B[0m\n\u001B[1;32m   1862\u001B[0m     writer \u001B[38;5;241m=\u001B[39m writer_class(\n\u001B[1;32m   1863\u001B[0m         features\u001B[38;5;241m=\u001B[39mwriter\u001B[38;5;241m.\u001B[39m_features,\n\u001B[1;32m   1864\u001B[0m         path\u001B[38;5;241m=\u001B[39mfpath\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSSSSS\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mshard_id\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m05d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mJJJJJ\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mjob_id\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m05d\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1867\u001B[0m         embed_local_files\u001B[38;5;241m=\u001B[39membed_local_files,\n\u001B[1;32m   1868\u001B[0m     )\n\u001B[1;32m   1869\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1870\u001B[0m     \u001B[43mwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1871\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CastError \u001B[38;5;28;01mas\u001B[39;00m cast_error:\n\u001B[1;32m   1872\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetGenerationCastError\u001B[38;5;241m.\u001B[39mfrom_cast_error(\n\u001B[1;32m   1873\u001B[0m         cast_error\u001B[38;5;241m=\u001B[39mcast_error,\n\u001B[1;32m   1874\u001B[0m         builder_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mbuilder_name,\n\u001B[1;32m   1875\u001B[0m         gen_kwargs\u001B[38;5;241m=\u001B[39mgen_kwargs,\n\u001B[1;32m   1876\u001B[0m         token\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoken,\n\u001B[1;32m   1877\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/datasets/arrow_writer.py:627\u001B[0m, in \u001B[0;36mArrowWriter.write_table\u001B[0;34m(self, pa_table, writer_batch_size)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_bytes \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mnbytes\n\u001B[1;32m    626\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_examples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mnum_rows\n\u001B[0;32m--> 627\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpa_writer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/pyarrow/ipc.pxi:529\u001B[0m, in \u001B[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/pyarrow/error.pxi:89\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/tts_torch/lib/python3.11/site-packages/fsspec/implementations/local.py:426\u001B[0m, in \u001B[0;36mLocalFileOpener.write\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    425\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrite\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 426\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T01:08:49.223032Z",
     "start_time": "2024-12-24T01:08:49.212569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ckpt = torch.load(\"checkpoints/hifigan_gen_universal.pth\", map_location=device, weights_only=True)\n",
    "print(ckpt.keys())"
   ],
   "id": "16adc97432a0ee74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['generator'])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T01:17:00.239704Z",
     "start_time": "2024-12-24T01:17:00.128210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint on the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ckpt = torch.load(\"checkpoints/checkpoints_23_12_2024_TTS_Transformer/transformer_tts_epoch24.pth\", map_location=device, weights_only=True)\n",
    "print(ckpt.keys())"
   ],
   "id": "2646fd639885d7ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['module.text_embedding.weight', 'module.pos_encoder.pe', 'module.mel_embed.weight', 'module.mel_embed.bias', 'module.pos_decoder.pe', 'module.encoder.transformer_encoder.layers.0.self_attn.in_proj_weight', 'module.encoder.transformer_encoder.layers.0.self_attn.in_proj_bias', 'module.encoder.transformer_encoder.layers.0.self_attn.out_proj.weight', 'module.encoder.transformer_encoder.layers.0.self_attn.out_proj.bias', 'module.encoder.transformer_encoder.layers.0.linear1.weight', 'module.encoder.transformer_encoder.layers.0.linear1.bias', 'module.encoder.transformer_encoder.layers.0.linear2.weight', 'module.encoder.transformer_encoder.layers.0.linear2.bias', 'module.encoder.transformer_encoder.layers.0.norm1.weight', 'module.encoder.transformer_encoder.layers.0.norm1.bias', 'module.encoder.transformer_encoder.layers.0.norm2.weight', 'module.encoder.transformer_encoder.layers.0.norm2.bias', 'module.encoder.transformer_encoder.layers.1.self_attn.in_proj_weight', 'module.encoder.transformer_encoder.layers.1.self_attn.in_proj_bias', 'module.encoder.transformer_encoder.layers.1.self_attn.out_proj.weight', 'module.encoder.transformer_encoder.layers.1.self_attn.out_proj.bias', 'module.encoder.transformer_encoder.layers.1.linear1.weight', 'module.encoder.transformer_encoder.layers.1.linear1.bias', 'module.encoder.transformer_encoder.layers.1.linear2.weight', 'module.encoder.transformer_encoder.layers.1.linear2.bias', 'module.encoder.transformer_encoder.layers.1.norm1.weight', 'module.encoder.transformer_encoder.layers.1.norm1.bias', 'module.encoder.transformer_encoder.layers.1.norm2.weight', 'module.encoder.transformer_encoder.layers.1.norm2.bias', 'module.encoder.transformer_encoder.layers.2.self_attn.in_proj_weight', 'module.encoder.transformer_encoder.layers.2.self_attn.in_proj_bias', 'module.encoder.transformer_encoder.layers.2.self_attn.out_proj.weight', 'module.encoder.transformer_encoder.layers.2.self_attn.out_proj.bias', 'module.encoder.transformer_encoder.layers.2.linear1.weight', 'module.encoder.transformer_encoder.layers.2.linear1.bias', 'module.encoder.transformer_encoder.layers.2.linear2.weight', 'module.encoder.transformer_encoder.layers.2.linear2.bias', 'module.encoder.transformer_encoder.layers.2.norm1.weight', 'module.encoder.transformer_encoder.layers.2.norm1.bias', 'module.encoder.transformer_encoder.layers.2.norm2.weight', 'module.encoder.transformer_encoder.layers.2.norm2.bias', 'module.encoder.transformer_encoder.layers.3.self_attn.in_proj_weight', 'module.encoder.transformer_encoder.layers.3.self_attn.in_proj_bias', 'module.encoder.transformer_encoder.layers.3.self_attn.out_proj.weight', 'module.encoder.transformer_encoder.layers.3.self_attn.out_proj.bias', 'module.encoder.transformer_encoder.layers.3.linear1.weight', 'module.encoder.transformer_encoder.layers.3.linear1.bias', 'module.encoder.transformer_encoder.layers.3.linear2.weight', 'module.encoder.transformer_encoder.layers.3.linear2.bias', 'module.encoder.transformer_encoder.layers.3.norm1.weight', 'module.encoder.transformer_encoder.layers.3.norm1.bias', 'module.encoder.transformer_encoder.layers.3.norm2.weight', 'module.encoder.transformer_encoder.layers.3.norm2.bias', 'module.decoder.transformer_decoder.layers.0.self_attn.in_proj_weight', 'module.decoder.transformer_decoder.layers.0.self_attn.in_proj_bias', 'module.decoder.transformer_decoder.layers.0.self_attn.out_proj.weight', 'module.decoder.transformer_decoder.layers.0.self_attn.out_proj.bias', 'module.decoder.transformer_decoder.layers.0.multihead_attn.in_proj_weight', 'module.decoder.transformer_decoder.layers.0.multihead_attn.in_proj_bias', 'module.decoder.transformer_decoder.layers.0.multihead_attn.out_proj.weight', 'module.decoder.transformer_decoder.layers.0.multihead_attn.out_proj.bias', 'module.decoder.transformer_decoder.layers.0.linear1.weight', 'module.decoder.transformer_decoder.layers.0.linear1.bias', 'module.decoder.transformer_decoder.layers.0.linear2.weight', 'module.decoder.transformer_decoder.layers.0.linear2.bias', 'module.decoder.transformer_decoder.layers.0.norm1.weight', 'module.decoder.transformer_decoder.layers.0.norm1.bias', 'module.decoder.transformer_decoder.layers.0.norm2.weight', 'module.decoder.transformer_decoder.layers.0.norm2.bias', 'module.decoder.transformer_decoder.layers.0.norm3.weight', 'module.decoder.transformer_decoder.layers.0.norm3.bias', 'module.decoder.transformer_decoder.layers.1.self_attn.in_proj_weight', 'module.decoder.transformer_decoder.layers.1.self_attn.in_proj_bias', 'module.decoder.transformer_decoder.layers.1.self_attn.out_proj.weight', 'module.decoder.transformer_decoder.layers.1.self_attn.out_proj.bias', 'module.decoder.transformer_decoder.layers.1.multihead_attn.in_proj_weight', 'module.decoder.transformer_decoder.layers.1.multihead_attn.in_proj_bias', 'module.decoder.transformer_decoder.layers.1.multihead_attn.out_proj.weight', 'module.decoder.transformer_decoder.layers.1.multihead_attn.out_proj.bias', 'module.decoder.transformer_decoder.layers.1.linear1.weight', 'module.decoder.transformer_decoder.layers.1.linear1.bias', 'module.decoder.transformer_decoder.layers.1.linear2.weight', 'module.decoder.transformer_decoder.layers.1.linear2.bias', 'module.decoder.transformer_decoder.layers.1.norm1.weight', 'module.decoder.transformer_decoder.layers.1.norm1.bias', 'module.decoder.transformer_decoder.layers.1.norm2.weight', 'module.decoder.transformer_decoder.layers.1.norm2.bias', 'module.decoder.transformer_decoder.layers.1.norm3.weight', 'module.decoder.transformer_decoder.layers.1.norm3.bias', 'module.decoder.transformer_decoder.layers.2.self_attn.in_proj_weight', 'module.decoder.transformer_decoder.layers.2.self_attn.in_proj_bias', 'module.decoder.transformer_decoder.layers.2.self_attn.out_proj.weight', 'module.decoder.transformer_decoder.layers.2.self_attn.out_proj.bias', 'module.decoder.transformer_decoder.layers.2.multihead_attn.in_proj_weight', 'module.decoder.transformer_decoder.layers.2.multihead_attn.in_proj_bias', 'module.decoder.transformer_decoder.layers.2.multihead_attn.out_proj.weight', 'module.decoder.transformer_decoder.layers.2.multihead_attn.out_proj.bias', 'module.decoder.transformer_decoder.layers.2.linear1.weight', 'module.decoder.transformer_decoder.layers.2.linear1.bias', 'module.decoder.transformer_decoder.layers.2.linear2.weight', 'module.decoder.transformer_decoder.layers.2.linear2.bias', 'module.decoder.transformer_decoder.layers.2.norm1.weight', 'module.decoder.transformer_decoder.layers.2.norm1.bias', 'module.decoder.transformer_decoder.layers.2.norm2.weight', 'module.decoder.transformer_decoder.layers.2.norm2.bias', 'module.decoder.transformer_decoder.layers.2.norm3.weight', 'module.decoder.transformer_decoder.layers.2.norm3.bias', 'module.decoder.transformer_decoder.layers.3.self_attn.in_proj_weight', 'module.decoder.transformer_decoder.layers.3.self_attn.in_proj_bias', 'module.decoder.transformer_decoder.layers.3.self_attn.out_proj.weight', 'module.decoder.transformer_decoder.layers.3.self_attn.out_proj.bias', 'module.decoder.transformer_decoder.layers.3.multihead_attn.in_proj_weight', 'module.decoder.transformer_decoder.layers.3.multihead_attn.in_proj_bias', 'module.decoder.transformer_decoder.layers.3.multihead_attn.out_proj.weight', 'module.decoder.transformer_decoder.layers.3.multihead_attn.out_proj.bias', 'module.decoder.transformer_decoder.layers.3.linear1.weight', 'module.decoder.transformer_decoder.layers.3.linear1.bias', 'module.decoder.transformer_decoder.layers.3.linear2.weight', 'module.decoder.transformer_decoder.layers.3.linear2.bias', 'module.decoder.transformer_decoder.layers.3.norm1.weight', 'module.decoder.transformer_decoder.layers.3.norm1.bias', 'module.decoder.transformer_decoder.layers.3.norm2.weight', 'module.decoder.transformer_decoder.layers.3.norm2.bias', 'module.decoder.transformer_decoder.layers.3.norm3.weight', 'module.decoder.transformer_decoder.layers.3.norm3.bias', 'module.fc_out.weight', 'module.fc_out.bias'])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b11748139ac7c18a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
